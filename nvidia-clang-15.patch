diff -up kernel/common/inc/nv-linux.h.2~ kernel/common/inc/nv-linux.h
--- kernel/common/inc/nv-linux.h.2~	2022-06-30 21:05:23.000000000 +0200
+++ kernel/common/inc/nv-linux.h	2022-09-05 01:14:32.950149402 +0200
@@ -42,6 +42,8 @@
   #include <generated/compile.h>
 #endif
 
+#include <linux/dma-direct.h>
+
 #include <linux/version.h>
 #include <linux/utsname.h>
 
diff -up kernel/conftest.sh.2~ kernel/conftest.sh
--- kernel/conftest.sh.2~	2022-06-30 20:31:05.000000000 +0200
+++ kernel/conftest.sh	2022-09-05 01:14:32.951149380 +0200
@@ -6,7 +6,11 @@ PATH="${PATH}:/bin:/sbin:/usr/bin"
 SCRIPTDIR=`dirname $0`
 cd $SCRIPTDIR
 
-CC="$1"
+# FIXME this script is so broken that it falls over additional
+# clang 15 strictness, so we use gcc for the config checks even
+# when building for the clang kernel.
+#CC="$1"
+CC=gcc
 ARCH=$2
 ISYSTEM=`$CC -print-file-name=include 2> /dev/null`
 SOURCES=$3
diff -up kernel/nvidia/nvlink_linux.c.2~ kernel/nvidia/nvlink_linux.c
--- kernel/nvidia/nvlink_linux.c.2~	2022-06-30 21:05:24.000000000 +0200
+++ kernel/nvidia/nvlink_linux.c	2022-09-05 01:14:32.951149380 +0200
@@ -568,7 +568,7 @@ void nvlink_assert(int cond)
     }
 }
 
-void * nvlink_allocLock()
+void * nvlink_allocLock(void)
 {
     struct semaphore *sema;
 
diff -up kernel/nvidia/os-interface.c.2~ kernel/nvidia/os-interface.c
--- kernel/nvidia/os-interface.c.2~	2022-06-30 21:05:23.000000000 +0200
+++ kernel/nvidia/os-interface.c	2022-09-05 01:15:31.897873508 +0200
@@ -1073,14 +1073,14 @@ void NV_API_CALL os_dbg_breakpoint(void)
 #endif // DEBUG
 }
 
-NvU32 NV_API_CALL os_get_cpu_number()
+NvU32 NV_API_CALL os_get_cpu_number(void)
 {
     NvU32 cpu_id = get_cpu();
     put_cpu();
     return cpu_id;
 }
 
-NvU32 NV_API_CALL os_get_cpu_count()
+NvU32 NV_API_CALL os_get_cpu_count(void)
 {
     return NV_NUM_CPUS();
 }
@@ -1133,7 +1133,7 @@ void NV_API_CALL os_get_screen_info(
 #endif
 }
 
-void NV_API_CALL os_dump_stack()
+void NV_API_CALL os_dump_stack(void)
 {
     dump_stack();
 }
diff -up kernel/nvidia-uvm/uvm_common.c.2~ kernel/nvidia-uvm/uvm_common.c
--- kernel/nvidia-uvm/uvm_common.c.2~	2022-06-30 21:05:24.000000000 +0200
+++ kernel/nvidia-uvm/uvm_common.c	2022-09-05 01:14:32.953149336 +0200
@@ -34,7 +34,7 @@ static int uvm_debug_prints = UVM_IS_DEB
 module_param(uvm_debug_prints, int, S_IRUGO|S_IWUSR);
 MODULE_PARM_DESC(uvm_debug_prints, "Enable uvm debug prints.");
 
-bool uvm_debug_prints_enabled()
+bool uvm_debug_prints_enabled(void)
 {
     return uvm_debug_prints != 0;
 }
diff -up kernel/nvidia-uvm/uvm_gpu_access_counters.c.2~ kernel/nvidia-uvm/uvm_gpu_access_counters.c
--- kernel/nvidia-uvm/uvm_gpu_access_counters.c.2~	2022-06-30 21:05:25.000000000 +0200
+++ kernel/nvidia-uvm/uvm_gpu_access_counters.c	2022-09-05 01:14:32.952149358 +0200
@@ -1524,7 +1524,7 @@ bool uvm_va_space_has_access_counter_mig
     return atomic_read(&va_space_access_counters->params.enable_mimc_migrations);
 }
 
-NV_STATUS uvm_perf_access_counters_init()
+NV_STATUS uvm_perf_access_counters_init(void)
 {
     uvm_perf_module_init("perf_access_counters",
                          UVM_PERF_MODULE_TYPE_ACCESS_COUNTERS,
@@ -1535,7 +1535,7 @@ NV_STATUS uvm_perf_access_counters_init(
     return NV_OK;
 }
 
-void uvm_perf_access_counters_exit()
+void uvm_perf_access_counters_exit(void)
 {
 }
 
diff -up kernel/nvidia-uvm/uvm_lock.c.2~ kernel/nvidia-uvm/uvm_lock.c
--- kernel/nvidia-uvm/uvm_lock.c.2~	2022-06-30 21:05:25.000000000 +0200
+++ kernel/nvidia-uvm/uvm_lock.c	2022-09-05 01:14:32.952149358 +0200
@@ -334,7 +334,7 @@ bool __uvm_check_all_unlocked(uvm_thread
     return false;
 }
 
-bool __uvm_thread_check_all_unlocked()
+bool __uvm_thread_check_all_unlocked(void)
 {
     return __uvm_check_all_unlocked(uvm_thread_context_lock_get());
 }
diff -up kernel/nvidia-uvm/uvm_migrate.c.2~ kernel/nvidia-uvm/uvm_migrate.c
--- kernel/nvidia-uvm/uvm_migrate.c.2~	2022-06-30 21:05:28.000000000 +0200
+++ kernel/nvidia-uvm/uvm_migrate.c	2022-09-05 01:15:46.457565254 +0200
@@ -792,7 +792,7 @@ static NV_STATUS uvm_migrate_release_use
     return NV_OK;
 }
 
-NV_STATUS uvm_migrate_init()
+NV_STATUS uvm_migrate_init(void)
 {
     NV_STATUS status = uvm_migrate_pageable_init();
     if (status != NV_OK)
@@ -818,7 +818,7 @@ NV_STATUS uvm_migrate_init()
     return NV_OK;
 }
 
-void uvm_migrate_exit()
+void uvm_migrate_exit(void)
 {
     uvm_migrate_pageable_exit();
 }
diff -up kernel/nvidia-uvm/uvm_perf_heuristics.c.2~ kernel/nvidia-uvm/uvm_perf_heuristics.c
--- kernel/nvidia-uvm/uvm_perf_heuristics.c.2~	2022-06-30 21:05:28.000000000 +0200
+++ kernel/nvidia-uvm/uvm_perf_heuristics.c	2022-09-05 01:14:32.953149336 +0200
@@ -28,7 +28,7 @@
 #include "uvm_gpu_access_counters.h"
 #include "uvm_va_space.h"
 
-NV_STATUS uvm_perf_heuristics_init()
+NV_STATUS uvm_perf_heuristics_init(void)
 {
     NV_STATUS status;
 
@@ -47,7 +47,7 @@ NV_STATUS uvm_perf_heuristics_init()
     return NV_OK;
 }
 
-void uvm_perf_heuristics_exit()
+void uvm_perf_heuristics_exit(void)
 {
     uvm_perf_access_counters_exit();
     uvm_perf_prefetch_exit();
diff -up kernel/nvidia-uvm/uvm_perf_prefetch.c.2~ kernel/nvidia-uvm/uvm_perf_prefetch.c
--- kernel/nvidia-uvm/uvm_perf_prefetch.c.2~	2022-06-30 21:05:28.000000000 +0200
+++ kernel/nvidia-uvm/uvm_perf_prefetch.c	2022-09-05 01:14:32.953149336 +0200
@@ -460,7 +460,7 @@ void uvm_perf_prefetch_unload(uvm_va_spa
     uvm_perf_module_unload(&g_module_prefetch, va_space);
 }
 
-NV_STATUS uvm_perf_prefetch_init()
+NV_STATUS uvm_perf_prefetch_init(void)
 {
     g_uvm_perf_prefetch_enable = uvm_perf_prefetch_enable != 0;
 
@@ -498,7 +498,7 @@ NV_STATUS uvm_perf_prefetch_init()
     return NV_OK;
 }
 
-void uvm_perf_prefetch_exit()
+void uvm_perf_prefetch_exit(void)
 {
     if (!g_uvm_perf_prefetch_enable)
         return;
diff -up kernel/nvidia-uvm/uvm_perf_thrashing.c.2~ kernel/nvidia-uvm/uvm_perf_thrashing.c
--- kernel/nvidia-uvm/uvm_perf_thrashing.c.2~	2022-06-30 21:05:28.000000000 +0200
+++ kernel/nvidia-uvm/uvm_perf_thrashing.c	2022-09-05 01:14:32.954149314 +0200
@@ -1952,7 +1952,7 @@ NV_STATUS uvm_perf_thrashing_register_gp
     return NV_OK;
 }
 
-NV_STATUS uvm_perf_thrashing_init()
+NV_STATUS uvm_perf_thrashing_init(void)
 {
     NV_STATUS status;
 
@@ -2011,7 +2011,7 @@ error:
     return status;
 }
 
-void uvm_perf_thrashing_exit()
+void uvm_perf_thrashing_exit(void)
 {
     cpu_thrashing_stats_exit();
 
diff -up kernel/nvidia-uvm/uvm_procfs.c.2~ kernel/nvidia-uvm/uvm_procfs.c
--- kernel/nvidia-uvm/uvm_procfs.c.2~	2022-06-30 21:05:25.000000000 +0200
+++ kernel/nvidia-uvm/uvm_procfs.c	2022-09-05 01:14:32.953149336 +0200
@@ -46,7 +46,7 @@ static struct proc_dir_entry *uvm_proc_d
 static struct proc_dir_entry *uvm_proc_gpus;
 static struct proc_dir_entry *uvm_proc_cpu;
 
-NV_STATUS uvm_procfs_init()
+NV_STATUS uvm_procfs_init(void)
 {
     if (!uvm_procfs_is_enabled())
         return NV_OK;
@@ -66,7 +66,7 @@ NV_STATUS uvm_procfs_init()
     return NV_OK;
 }
 
-void uvm_procfs_exit()
+void uvm_procfs_exit(void)
 {
     uvm_procfs_destroy_entry(uvm_proc_dir);
 }
@@ -95,12 +95,12 @@ void uvm_procfs_destroy_entry(struct pro
     procfs_destroy_entry_with_root(entry, entry);
 }
 
-struct proc_dir_entry *uvm_procfs_get_gpu_base_dir()
+struct proc_dir_entry *uvm_procfs_get_gpu_base_dir(void)
 {
     return uvm_proc_gpus;
 }
 
-struct proc_dir_entry *uvm_procfs_get_cpu_base_dir()
+struct proc_dir_entry *uvm_procfs_get_cpu_base_dir(void)
 {
     return uvm_proc_cpu;
 }
diff -up kernel/nvidia-uvm/uvm_push.c.2~ kernel/nvidia-uvm/uvm_push.c
--- kernel/nvidia-uvm/uvm_push.c.2~	2022-06-30 21:05:26.000000000 +0200
+++ kernel/nvidia-uvm/uvm_push.c	2022-09-05 01:14:32.953149336 +0200
@@ -242,12 +242,12 @@ NV_STATUS __uvm_push_begin_acquire_on_ch
     return status;
 }
 
-bool uvm_push_info_is_tracking_descriptions()
+bool uvm_push_info_is_tracking_descriptions(void)
 {
     return uvm_debug_enable_push_desc != 0;
 }
 
-bool uvm_push_info_is_tracking_acquires()
+bool uvm_push_info_is_tracking_acquires(void)
 {
     return uvm_debug_enable_push_acquire_info != 0;
 }
diff -up kernel/nvidia-uvm/uvm_thread_context.c.2~ kernel/nvidia-uvm/uvm_thread_context.c
--- kernel/nvidia-uvm/uvm_thread_context.c.2~	2022-06-30 21:05:26.000000000 +0200
+++ kernel/nvidia-uvm/uvm_thread_context.c	2022-09-05 01:14:32.953149336 +0200
@@ -101,7 +101,7 @@ static DEFINE_PER_CPU(uvm_thread_context
 static void thread_context_non_interrupt_remove(uvm_thread_context_t *thread_context,
                                                 uvm_thread_context_table_entry_t *thread_context_entry);
 
-bool uvm_thread_context_wrapper_is_used()
+bool uvm_thread_context_wrapper_is_used(void)
 {
     // The wrapper contains lock information. While uvm_record_lock_X
     // routines are a no-op outside of debug mode, unit tests do invoke their
diff -up kernel/nvidia-uvm/uvm_tools.c.2~ kernel/nvidia-uvm/uvm_tools.c
--- kernel/nvidia-uvm/uvm_tools.c.2~	2022-06-30 21:05:25.000000000 +0200
+++ kernel/nvidia-uvm/uvm_tools.c	2022-09-05 01:14:32.952149358 +0200
@@ -2141,7 +2141,7 @@ NV_STATUS uvm_api_tools_get_processor_uu
     return NV_OK;
 }
 
-void uvm_tools_flush_events()
+void uvm_tools_flush_events(void)
 {
     tools_schedule_completed_events();
 
